
import React, { useState, useEffect, useRef } from 'react';
import { GoogleGenAI, LiveServerMessage, Modality } from '@google/genai';
import { UserProfile, UserAgeGroup } from '../../types';
import { ADMIN_PHONE, DEVELOPER } from '../../constants';

function encode(bytes: Uint8Array) {
  let binary = '';
  const len = bytes.byteLength;
  for (let i = 0; i < len; i++) binary += String.fromCharCode(bytes[i]);
  return btoa(binary);
}

function decode(base64: string) {
  const binaryString = atob(base64);
  const bytes = new Uint8Array(binaryString.length);
  for (let i = 0; i < binaryString.length; i++) bytes[i] = binaryString.charCodeAt(i);
  return bytes;
}

async function decodeAudioData(data: Uint8Array, ctx: AudioContext, sampleRate: number, numChannels: number): Promise<AudioBuffer> {
  const dataInt16 = new Int16Array(data.buffer);
  const frameCount = dataInt16.length / numChannels;
  const buffer = ctx.createBuffer(numChannels, frameCount, sampleRate);
  for (let channel = 0; channel < numChannels; channel++) {
    const channelData = buffer.getChannelData(channel);
    for (let i = 0; i < frameCount; i++) channelData[i] = dataInt16[i * numChannels + channel] / 32768.0;
  }
  return buffer;
}

const RobotVoice: React.FC<{ user: UserProfile; onClose: () => void }> = ({ user, onClose }) => {
  const [status, setStatus] = useState<'idle' | 'connecting' | 'listening' | 'speaking'>('idle');
  const [visualData, setVisualData] = useState<string | null>(null);
  const [isReady, setIsReady] = useState(false);
  
  const audioContextRef = useRef<AudioContext | null>(null);
  const outputAudioContextRef = useRef<AudioContext | null>(null);
  const sessionRef = useRef<any>(null);
  const nextStartTimeRef = useRef(0);
  const sourcesRef = useRef<Set<AudioBufferSourceNode>>(new Set());

  const startSession = async () => {
    setIsReady(true);
    setStatus('connecting');
    
    audioContextRef.current = new (window.AudioContext || (window as any).webkitAudioContext)({ sampleRate: 16000 });
    outputAudioContextRef.current = new (window.AudioContext || (window as any).webkitAudioContext)({ sampleRate: 24000 });
    await audioContextRef.current.resume();
    await outputAudioContextRef.current.resume();

    const ai = new GoogleGenAI({ apiKey: process.env.API_KEY || '' });
    const stream = await navigator.mediaDevices.getUserMedia({ audio: true });

    const instruction = `Ø£Ù†Øª Ø±ÙÙŠÙ‚ Ø°ÙƒÙŠ ÙˆØ­ÙƒÙŠÙ… Ø¬Ø¯Ø§Ù‹ ÙŠØ¯Ø¹Ù‰ "${user.robotName || 'Ø¹Ù„ÙŠ'}". 
Ø£Ù†Øª Ø§Ù„Ø¹Ù‚Ù„ Ø§Ù„Ù…ÙÙƒØ± Ù„ØªØ·Ø¨ÙŠÙ‚ 'ØµØ¯ÙŠÙ‚Ùƒ Ø§Ù„Ù…Ø³Ù„Ù…' Ù…Ù† ØªØ·ÙˆÙŠØ± ${DEVELOPER}.

Ø£Ù†Øª ØªØªØ­Ø¯Ø« ØµÙˆØªÙŠØ§Ù‹ Ø§Ù„Ø¢Ù† Ù…Ø¹:
- ${user.name}ØŒ Ù…Ù† ${user.location.city}ØŒ Ø¹Ù…Ø±Ù‡ ${user.ageGroup}.
- Ø­Ø§Ù„ØªÙ‡ Ø§Ù„Ø¥ÙŠÙ…Ø§Ù†ÙŠØ©: ${user.isNewToIslam ? 'Ø¬Ø¯ÙŠØ¯ Ø¹Ù„Ù‰ Ø§Ù„Ø¥Ø³Ù„Ø§Ù… (ÙƒÙ† Ù…Ù‚Ù†Ø¹Ø§Ù‹ ÙˆØ¹Ù„Ù…ÙŠØ§Ù‹ Ø¨Ø±ÙÙ‚)' : 'Ù…Ø³Ù„Ù… Ù…Ù…Ø§Ø±Ø³'}.
- ÙˆØ¶Ø¹ Ø§Ù„Ø·ÙÙ„: ${user.isChildMode ? 'Ù…ÙØ¹Ù‘Ù„ (ÙƒÙ† Ù…Ø±Ø­Ø§Ù‹ Ø¬Ø¯Ø§Ù‹ ÙˆØ§Ø³ØªØ®Ø¯Ù… Ø¹Ø¨Ø§Ø±Ø§Øª ØªØ´Ø¬ÙŠØ¹ÙŠØ© Ù„Ù„Ø£Ø·ÙØ§Ù„)' : 'ØºÙŠØ± Ù…ÙØ¹Ù‘Ù„'}.

Ø£Ù†Øª ØªØ¹Ø±Ù ÙƒÙ„ Ø´ÙŠØ¡ ÙÙŠ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚:
- ÙŠÙˆØ¬Ø¯ Ù‚Ø³Ù… Ù„Ù„Ù‚Ø±Ø¢Ù† Ø§Ù„ÙƒØ±ÙŠÙ…ØŒ Ø§Ù„Ø£Ø°ÙƒØ§Ø±ØŒ Ù…ÙˆØ§Ù‚ÙŠØª Ø§Ù„ØµÙ„Ø§Ø©ØŒ ÙˆØªÙ‚ÙˆÙŠÙ… Ø§Ù„ØµÙŠØ§Ù….
- Ù‡Ù†Ø§Ùƒ Ù…ÙŠØ²Ø© Ø§Ù„Ø±Ø¨Ø· Ø§Ù„Ø¹Ø§Ø¦Ù„ÙŠ ÙˆØ§Ù„ØªØ­ÙƒÙ… ÙÙŠ Ø§Ù„ØªÙ„ÙØ§Ø².
- Ø§Ù„Ø¯Ø¹Ù… Ø§Ù„ÙÙ†ÙŠ ÙˆØ§Ù„Ù…Ø§Ø¯ÙŠ ÙŠØªÙ… Ø¹Ø¨Ø± Ø§Ù„Ù…Ø´Ø±Ù Ø¹Ù„ÙŠ Ø·Ù‡ (${ADMIN_PHONE}).

Ø¹Ù†Ø¯Ù…Ø§ ØªØ´Ø±Ø­ Ø´ÙŠØ¦Ø§Ù‹ Ù…Ø¹Ù‚Ø¯Ø§Ù‹ØŒ Ø§Ø±Ø³Ù… Ø´ÙƒÙ„Ø§Ù‹ ØªÙˆØ¶ÙŠØ­ÙŠØ§Ù‹ Ø¨Ù€ SVG Ø¯Ø§Ø®Ù„ ÙˆØ³Ù… <drawing>...</drawing>.
ØªØ­Ø¯Ø« Ø¨Ø§Ø®ØªØµØ§Ø± Ù„Ø£Ù†Ùƒ ÙÙŠ ÙˆØ¶Ø¹ Ø§Ù„ØµÙˆØªØŒ ÙˆÙƒÙ† Ø¯Ø§Ø¦Ù…Ø§Ù‹ Ù…Ø­ÙØ²Ø§Ù‹ ÙˆØ¥ÙŠØ¬Ø§Ø¨ÙŠØ§Ù‹.`;

    const sessionPromise = ai.live.connect({
      model: 'gemini-2.5-flash-native-audio-preview-12-2025',
      callbacks: {
        onopen: () => {
          setStatus('listening');
          const source = audioContextRef.current!.createMediaStreamSource(stream);
          const processor = audioContextRef.current!.createScriptProcessor(4096, 1, 1);
          processor.onaudioprocess = (e) => {
            const inputData = e.inputBuffer.getChannelData(0);
            const int16 = new Int16Array(inputData.length);
            for (let i = 0; i < inputData.length; i++) int16[i] = inputData[i] * 32768;
            sessionPromise.then(s => s.sendRealtimeInput({ media: { data: encode(new Uint8Array(int16.buffer)), mimeType: 'audio/pcm;rate=16000' } }));
          };
          source.connect(processor);
          processor.connect(audioContextRef.current!.destination);
        },
        onmessage: async (message: LiveServerMessage) => {
          if (message.serverContent?.outputTranscription) {
            const text = message.serverContent.outputTranscription.text;
            const drawingMatch = text.match(/<drawing>([\s\S]*?)<\/drawing>/);
            if (drawingMatch) setVisualData(drawingMatch[1]);
          }

          const base64Audio = message.serverContent?.modelTurn?.parts[0]?.inlineData?.data;
          if (base64Audio && outputAudioContextRef.current) {
            setStatus('speaking');
            nextStartTimeRef.current = Math.max(nextStartTimeRef.current, outputAudioContextRef.current.currentTime);
            const buffer = await decodeAudioData(decode(base64Audio), outputAudioContextRef.current, 24000, 1);
            const source = outputAudioContextRef.current.createBufferSource();
            source.buffer = buffer;
            source.connect(outputAudioContextRef.current.destination);
            source.start(nextStartTimeRef.current);
            nextStartTimeRef.current += buffer.duration;
            sourcesRef.current.add(source);
            source.onended = () => {
                sourcesRef.current.delete(source);
                if (sourcesRef.current.size === 0) setStatus('listening');
            };
          }
        },
        onerror: (e) => { console.error(e); setStatus('idle'); },
        onclose: () => onClose()
      },
      config: {
        responseModalities: [Modality.AUDIO],
        outputAudioTranscription: {},
        speechConfig: {
          voiceConfig: {
            prebuiltVoiceConfig: { voiceName: 'Zephyr' },
          },
        },
        systemInstruction: instruction
      }
    });
    sessionRef.current = await sessionPromise;
  };

  useEffect(() => {
    return () => {
      sessionRef.current?.close();
      audioContextRef.current?.close();
      outputAudioContextRef.current?.close();
    };
  }, []);

  return (
    <div className="fixed inset-0 z-[200] bg-teal-950 flex flex-col items-center p-6 text-center font-['Cairo'] overflow-hidden">
      <div className="w-full flex justify-between items-center mb-10 shrink-0">
        <span className="text-white/20 text-[10px] font-black tracking-widest uppercase">AliTech Intelligence Engine</span>
        <button onClick={onClose} className="w-10 h-10 rounded-full bg-white/10 flex items-center justify-center text-white">âœ•</button>
      </div>

      {!isReady ? (
        <div className="flex-1 flex flex-col items-center justify-center space-y-8 animate-in zoom-in-95">
           <div className="w-40 h-40 bg-white rounded-[3rem] flex items-center justify-center text-8xl shadow-2xl border-4 border-[#d4af37]/20">ğŸ¤–</div>
           <h2 className="text-2xl font-black text-white">ØªØ­Ø¯Ø« Ø§Ù„Ø¢Ù† Ù…Ø¹ {user.robotName || 'Ø¹Ù„ÙŠ'}</h2>
           <p className="text-stone-400 text-xs px-10">Ø³ÙŠÙƒÙˆÙ† "Ø¹Ù„ÙŠ" Ù…Ø¯Ø±ÙƒØ§Ù‹ ØªÙ…Ø§Ù…Ø§Ù‹ Ù„Ù…ÙˆÙ‚Ø¹Ùƒ ÙˆØµÙŠØ§Ù…Ùƒ ÙˆØµÙ„Ø§ØªÙƒ Ù„ÙŠÙ‚Ø¯Ù… Ù„Ùƒ Ø£ÙØ¶Ù„ Ù†ØµÙŠØ­Ø©.</p>
           <button onClick={startSession} className="bg-emerald-500 text-white px-10 py-5 rounded-[2rem] font-black text-lg shadow-xl active:scale-95 transition-all">Ø§Ø¨Ø¯Ø£ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø© ğŸ™ï¸</button>
        </div>
      ) : (
        <div className="flex-1 w-full flex flex-col items-center gap-8 overflow-hidden">
           <div className={`w-full max-w-sm rounded-[3rem] transition-all duration-700 overflow-hidden ${visualData ? 'flex-1 bg-white p-4 shadow-2xl' : 'h-0 opacity-0'}`}>
              {visualData && <div className="w-full h-full flex items-center justify-center" dangerouslySetInnerHTML={{ __html: visualData }} />}
           </div>

           <div className="flex flex-col items-center shrink-0">
              <div className={`w-36 h-36 rounded-[2.5rem] bg-white flex items-center justify-center shadow-2xl relative transition-all duration-500 ${status === 'speaking' ? 'scale-110' : 'scale-100'}`}>
                 <span className="text-7xl">ğŸ¤–</span>
                 {status === 'speaking' && <div className="absolute inset-0 rounded-[2.5rem] border-4 border-emerald-400 animate-ping opacity-20"></div>}
              </div>
              <h3 className="text-xl font-black text-white mt-6">{user.robotName || 'Ø¹Ù„ÙŠ'}</h3>
              <p className="text-[10px] text-emerald-400 font-bold uppercase tracking-widest mt-1">
                {status === 'connecting' ? 'Ø¬Ø§Ø±ÙŠ Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨Ø§Ù„Ø¹Ù‚Ù„ Ø§Ù„Ù…ÙÙƒØ±...' : status === 'listening' ? 'Ø£Ø³Ù…Ø¹Ùƒ ÙŠØ§ Ø¨Ø·Ù„...' : 'Ø£Ø¬ÙŠØ¨Ùƒ Ø§Ù„Ø¢Ù†...'}
              </p>
           </div>

           <div className="flex gap-2 h-10 items-end shrink-0">
             {[1,2,3,4,5].map(i => (
               <div key={i} className={`w-1.5 rounded-full bg-emerald-400 transition-all duration-300 ${status === 'speaking' ? 'animate-bounce' : 'opacity-20'}`} 
                 style={{ height: status === 'speaking' ? `${20 + Math.random()*20}px` : '10px', animationDelay: `${i*0.1}s` }} />
             ))}
           </div>
        </div>
      )}

      <div className="mt-10 mb-4 opacity-20 text-white text-[8px] font-black uppercase tracking-[0.5em]">Global Context Aware Engine</div>
    </div>
  );
};

export default RobotVoice;
